# -*- coding: utf-8 -*-
"""series de tiempo 2.5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U67iVAiDEei6fuqnCZDeVW7PwyMVh3eL

<font color="Teal" face="Comic Sans MS,arial">
<h1 align="center"><i>PROYECTO:SERIES DE TIEMPO</i></h1></font>
<font color="Black" face="Comic Sans MS,arial">
<h5 align="center"><i>Creado por:</i><h5 align="center"><i> -Robles Salazar Diego   318311984 </i><h5

##**OBJETIVO**

Predecir los precios futuros de los muebles utilizando una serie de tiempo de precios históricos de una mueblería.

Utilizando ARIMA:

Aplicar el modelo ARIMA para ajustar y predecir la serie de tiempo de precios de muebles.
Identificar los parámetros óptimos del modelo ARIMA mediante técnicas de selección de orden, como AIC (Akaike Information Criterion) o BIC (Bayesian Information Criterion).
Evaluar la precisión y el rendimiento del modelo ARIMA utilizando métricas de evaluación, como el error medio absoluto (MAE) o el error cuadrado medio (MSE).
Utilizando AutoARIMA:

Aplicar el algoritmo AutoARIMA para determinar automáticamente los mejores parámetros del modelo ARIMA para la serie de tiempo de precios de muebles.
Ajustar y predecir la serie de tiempo utilizando el modelo ARIMA determinado por AutoARIMA.
Comparar y evaluar los resultados de AutoARIMA con los obtenidos por el enfoque manual utilizando ARIMA.
Utilizando una red neuronal:

Diseñar y entrenar una red neuronal, como una red neuronal recurrente (RNN) o una red neuronal convolucional (CNN), para modelar la serie de tiempo de precios de muebles.
Utilizar los datos históricos de precios de muebles como entrada para la red neuronal y predecir los precios futuros.
Ajustar los hiperparámetros de la red neuronal, como la arquitectura de capas, la función de activación y la tasa de aprendizaje, para optimizar el rendimiento del modelo.
Evaluar la precisión y el rendimiento del modelo de red neuronal utilizando métricas de evaluación y comparar los resultados con los modelos ARIMA y AutoARIMA.

## **Contenido**

1. **Introducción a Serie de tiempo**
        1.1 ¿Qué es una serie de tiempo?
       
2. **Evaluación de Modelos**
   
3. **Modelo Arima**
        3.1 Definición Matemática de Modelo Arima
        3.2 Leemos datos
        3.3 Análisis Exploratorio de Datos (EDA)
        3.4 Prueba de Dickey Fuller Aumentada (ADF)
        3.5 División de datos para entrenamiento y prueba
        3.6 Modelo con Auto-Arima
        3.7 Implementación del modelo
4. **Modelo LSTM**
        4.1 Estandarización de los datos
        4.2 Modelación con LSTM
        4.3 Evaluación del modelo
5. **Modelo Random Forest**
        5.1 Idea intuitiva detrás de los Bosques Aleatorios
        5.2 Feature Change
        5.3 Implementación del Modelo Random Forest
        5.4 Evaluación del modelo
6. **Modelo Prophet**
        6.1 Modelación del modelo Prophet
        6.2 Evaluación del modelo
7. **Conclusión**

## 1.1 **¿Qué es una Serie de Tiempo?**

Una serie de tiempo es una secuencia de datos u observaciones, medidos en determinados momentos y ordenados cronológicamente. Visualmente, es una curva que evoluciona en el tiempo. Una serie de tiempo es un conjunto de observaciones sobre los valores que toma una variable (cuantitativa) a través del tiempo.

**Ejemplo de algunas aplicaciones de datos de serie temporal**

1. Anual: - PIB, serie macroeconómica
2. Trimestral:- Ingresos de una empresa.
3. Mensual: - Ventas, gastos, salario
4. Semanal:- Demanda, Precio de Gasolina y Diesel
5. Diariamente:- Precio de cierre de acciones, valor sensex, transacción diaria de cajero automático
6. Por hora: - AAQI

El análisis de series de tiempo puede ser útil para ver cómo cambia un activo, un valor o una variable económica dados con el tiempo. También se puede utilizar para examinar cómo los cambios asociados con el punto de datos elegido se comparan con los cambios en otras variables durante el mismo período de tiempo.

#Caracteristicas de la serie de tiempo
Una serie de temporal es una coleccion de puntos de datos que se almacenan con respecto a su tiempo.El analisis matematico y estadistico realizado es este tipo de datos para encontar patrones ocultos e informacion significativa se denomina analisis de series temporales.Las tecnicas de modelacion de series temporales  se usan para entender patrones pasados apartir de los datos y tratar de pronosticar futuros(horizone futuro).Estas tecnica y metodologias han ido evolucionanado durante decadas.

Las observaciones con marcas de tiempo continuas y variables objetivo a veces se enmarcan como problemas de regresion sencillos al descomponer las fechas en minutos,horas,dias,semanas,meses,años,etc., que no es la forma correcta de manejar dichos datos porque los resultados obtenidos son deficientes.Conforme vayamos avanzando en este proyecto se dara el enfoque correcto para poder manejar los datos de series de tiempo.

Existen diferentes tipos de datos como bien son las estructurados,semiestructurados y no estructurados, y cada uno se maneja de manera distinta esto para obtener el maximo aprovechamiento y conocimiento.trataremos de integrar datos de series de tiempo del mercado de valores,clima,tasas de natalidad,trafico,eentre otros.

#Supuesto de las series de tiempo
Algunos de los supuestos de las series de tiempo se basan en el sentido comun.Pero siempre se debe de tener en cuenta algunas cosas
>**!Los pronosticos a muy largo plazo no funcionan bien!**(se distirciona)



*   El pronostico se realiza teniendo en cuenta que el mercado y las demas condiciones no van a cambiar el futuro.
*   No habra ningun cambio en el mercado.
*   El cambio es gradual y no un cambio drastico
*   Eventos como la desmonetizacion desbaratarian los pronosticos.

Con base a los datos disponibles, no deberiamos intentar pronosticar mas de unos pocos periodos por delante.

# Tipos de series de tiempo
##Series temporales univariables
Es una serie de datos con una unica variable dependiente del tiempo, como la demanda de un producto en el tiempo t.
>Una serie de tiempo que consta de observaciones individuales (escalares) registradas secuencialmente en incrementos de tiempo iguales. Algunos ejemplos son las concentraciones mensuales de CO2 y las oscilaciones del sur para predecir los efectos de El Niño.

Por ejemplo, eche un vistazo al conjunto de datos de muestra a continuación que consta de las temperaturas minimas durante los meses del año desde el hemisferio sur desde 1981 hasta 1990. Aquí, la temperatura es la variable dependiente (dependiente del tiempo).

##Series temporales multivariadas

Los datos de una serie temporal multivariada contienen más de una variable dependiente del tiempo. Cada variable aquí depende no solo de los valores pasados, sino que también tiene cierta dependencia de otras variables. Esta dependencia se utiliza para pronosticar los valores futuros

# 2. Evaluación de Modelos

Al desarrollar modelos de aprendizaje automático, generalmente comparamos varios modelos durante la fase de construcción. Luego, estimamos los rendimientos de esos modelos y se seleccionamos el modelo que considera que tiene mayor probabilidades de funcionar bien. Necesitamos medidas objetivas de desempeño para poder decidir qué pronóstico conservar como su pronóstico real. A lo largo de este cuaderno, vamos a usar numerosas herramientas para la evaluación de modelos. Veremos diferentes estrategias para evaluar modelos de aprendizaje automático en general y adaptaciones y consideraciones específicas a tener en cuenta para la previsión.

#Metricas
##Metrica 1:MSE
El error cuadrático medio (MSE) es una de las métricas más utilizadas en el aprendizaje automático. Se calcula como el promedio de los errores al cuadrado. Para calcular el MSE, toma los errores por fila de datos, eleva al cuadrado esos errores y luego toma el promedio de ellos.

<h5 align="center">MSE = (1/n) * Σ(yᵢ - ȳ)²<i></i></h5>

Donde:

MSE: Mean Squared Error (Error Cuadrático Medio)

n: número de muestras de datos

yᵢ: valor real observado de la i-ésima muestra

ȳ: valor medio de los valores reales observados


La métrica de error MSE es excelente para comparar diferentes modelos en el mismo conjunto de datos. La escala del MSE será la misma para cada modelo aplicado al mismo conjunto de datos. Sin embargo, la escala de la métrica no es muy intuitiva, lo que dificulta su interpretación fuera de la evaluación comparativa de multiples modelos.

##Métrica 2: RMSE

El RMSE, o raíz del error cuadrático medio, es la raiz cuadrada del error cuadrático medio. Como puede comprender, sacar la raiz cuadrada del MSE no hace ninguna diferencia cuando desea utilizar las métricas de error para clasificar los rendimientos en orden.

<h5 align="center">RSME = &radic;MSE<i></i></h5>

Sin embargo, hay una ventaja en usar el RMSE en lugar del MSE La razón para sacar la raíz cuadrada del MSE es que la escala del RMSE es la misma que la escala de la variable original. En la fórmula MSE, se toma el promedio de los errores al cuadrado. Esto hace que el valor sea dificil de interpretar. El uso de la raiz cuadrada hará. que la escala de la métrica de error vuelva a la escala de sus valores reales.

##Métrica 3: MAE

El error absoluto medio (MAE) se calcula tomando las diferencias absolutas entre los valores pronosticados y reales por fila. El promedio de esos errores absolutos es el error absoluto medio.

<h5 align="center">MAE= (1/n) * Σ|yᵢ - ȳ|<i></i></h5>

El MAE toma los valores absolutos de los errores antes de promediarios. Tomar el promedio de los errores absolutos es una forma de asegurarse de que la suma de los errores no haga que se cancelen entre sí.

Has visto que el MSE usa el cuadrado de los errores para evitar esto, y el MAE es una alternativa a esto. El MAE tiene una formula más intuitiva: es la métrica de error que la mayoría de la gente encuentra intuitivamente. Sin embargo, el RMSE es generalmente favorecido sobre el MAE.

##Métrica 4: MAPE

EI MAPE abreviatura de Error porcentual absoluto medio, se calcula tomando el error de cada predicción, dividido por el valor real. Esto se hace para obtener los errores relativos a los valores reales. Esto hará que la medida del error sea un porcentaje y, por lo tanto, está estandarizado.

Como hemos entendido de las medidas de error anteriores, no se estandarizaron en una escala entre cero y uno. Sin embargo, esta estandarización es muy útil. Esto

facilita la comunicación de los resultados de rendimiento.

Para calcular el MAPE, toma los valores absolutos de esos porcentajes por fila y calcula su promedio.

<h5 align="center">MAPE= (1/n) * Σ|(yᵢ - ȳ)/yᵢ|<i></i></h5>

EI MAPE mide un porcentaje de error. Es una medida de error, por lo que los valores más bajos para el MAPE son mejores. Sin embargo, puede convertir facilmente el MAPE en una medida de bondad de ajuste calculando "1-MAPE". En muchos casos, es más fácil comunicar el desempeño en términos de un resultado positivo que negativo.

## Métrica 5: R²

La mètrica R2 (R cuadrado) es una métrica que está muy cerca de la métrica 1 - MAPE. Es una metrica de rendimiento en lugar de una métrica de error, lo que la hace ideal para estimar el rendimiento de nuestro modelo.

El R² es un valor que tiende a estar entre 0 y 1, siendo 0 malo y 1 perfecto. Por lo tanto, se puede usar fácilmente como un porcentaje multiplicándolo por 100. El único caso en el que el R² puede ser negativo es si su pronóstico es más del 100% incorrecto.

<h5 align="center">R²=1- * Σ(yᵢ - ȳ)² / Σ(yᵢ - ȳ)²<i></i></h5>

La formula hace un cálculo interesante. Calcula una relación entre la suma de los errores al cuadrado y la suma de las desviaciones entre el pronóstico y el promedio. Esto se reduce a un porcentaje de aumento de su modelo sobre el uso del promedio como modelo. Si su modelo es una predicción tan mala como usar el promedio, entonces el R2 será cero. Como el promedio se usa a menudo como modelo de referencia, esta es una métrica de rendimiento muy práctica.

## 2.6 Función para evaluar
"""

def evaluacion_metrica(y_true, y_pred):

    def mean_absolute_percentage_error(y_true, y_pred):
        y_true, y_pred = np.array(y_true), np.array(y_pred)
        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    print('Evaluation metric results:-')
    print(f'MSE is : {metrics.mean_squared_error(y_true, y_pred)}')
    print(f'MAE is : {metrics.mean_absolute_error(y_true, y_pred)}')
    print(f'RMSE is : {np.sqrt(metrics.mean_squared_error(y_true, y_pred))}')
    print(f'MAPE is : {mean_absolute_percentage_error(y_true, y_pred)}')
    print(f'R2 is : {metrics.r2_score(y_true, y_pred)}',end='\n\n')

"""# 3. Pronóstico de Series de tiempo

En este cuaderno intentaremos pronosticar una serie de datos de tiempo básicamente. Construiremos cuatro modelos diferentes con Python e inspeccionaremos sus resultados. Los modelos que utilizaremos son ARIMA (media móvil integrada autorregresiva), LSTM (red neuronal de memoria a largo plazo), Random Forest y Facebook Prophet.
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Manipulación y tratamiento de Datos
import numpy as np
import pandas as pd

# Visualización de datos
import plotly.express as px
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('ggplot')

# Modelación Arima
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller

# Métrica de Evaluación
from sklearn.metrics import mean_squared_error
from statsmodels.tools.eval_measures import rmse
from sklearn import metrics

# No presentar advertencias
import warnings
warnings.filterwarnings("ignore")

"""## 3.1 ARIMA (Autoregressive Integrated Moving Average)

ARIMA es un modelo que se utiliza para predecir tendencias futuras en una serie de datos de tiempo. Es un modelo que forma de análisis de regresión. El modelo Arima esta compuesto por 3 término:
* **AR (Autoregresión) :** Modelo que muestra una variable cambiante que retrocede sobre sus propios valores atrasados/anteriores.
* **I (Integrado) :** Diferenciación de observaciones sin procesar para permitir que la serie temporal se vuelva estacionaria
* **MA (Promedio móvil) :** Dependencia entre una observación y un error residual de un modelo de promedio móvil

Para los modelos ARIMA, una notación estándar sería ARIMA con p, d y q, donde los valores enteros sustituyen a los parámetros para indicar el tipo de modelo ARIMA utilizado.

* **p:** el número de observaciones de retraso en el modelo; también conocido como el orden de retardo.
* **d:** el número de veces que se diferencian las observaciones sin procesar; también conocido como el grado de diferenciación.
* **q:** el tamaño de la ventana de promedio móvil; también conocido como el orden de la media móvil.

### Definición del modelo
El modelo ARIMA es la abreviatura del modelo Autoregresivo de Media Móvil Integrada. La diferencia con el modelo ARMA es pequeña: solo hay un efecto adicional que hace que la serie temporal no sea estacionaria. Un ejemplo simple de esto sería una tendencia lineal creciente, como se muestra en la siguiente ecuación:

$$X_t= c+\epsilon_t+ \sum_{i=1}^{p}\phi_i X_{t-i}+\sum_{i=1}^{q} \theta_i \epsilon_{t-i}+\delta_t $$

Al diferenciar una serie temporal, en realidad comienza a modelar las diferencias de un paso a otro en lugar de los valores originales. Si los valores reales de una variable no son estables en el tiempo, aún es posible que las diferencias sean estables en el tiempo.

La tendencia lineal es un gran ejemplo de esto. Imagine una tendencia lineal que comienza desde 0 y se incrementa en 2 cada paso de tiempo. Los valores no serán para nada estacionarios: aumentarán infinitamente. Sin embargo, la diferencia entre cada valor y el siguiente es siempre 2, por lo que la serie temporal diferenciada es perfectamente estacionaria.

## 3.2 Leemos los datos
"""

df = pd.read_csv("/content/drive/MyDrive/_Curso Serie de tiempo/data/df_furniture.csv")
df.head()

"""## 3.3 Análisis Exploratorio de Datos"""

df.info()

"""Como podemos ver la variable `Month` es una variabe del tipo objeto, debemos convertir esa variable a un formato de tiempo."""

# Convertir a un formato de datetime
from datetime import datetime
df['Month'] = pd.to_datetime(df['Month'])

df.info()

df = df.set_index("Month")
df.head()

df.index.freq = 'MS'

df.head()

fig = px.line(df, x=df.index, y="furniture_price_adjusted",template = "plotly_dark",
              title="Precio de Venta Furniture")
fig.show()

"""Cuando miramos el gráfico, podemos ver que hay una estacionalidad en los datos. Es por eso que usaremos SARIMA (Seasonal ARIMA) en lugar de ARIMA.

## 3.4 La prueba de Dickey-Fuller aumentada
Una prueba de Dickey-Fuller aumentada (ADF) es un tipo de prueba estadística que determina si una raíz unitaria está presente en los datos de series de tiempo. Las raíces unitarias pueden causar resultados impredecibles en el análisis de series de tiempo. Se forma una hipótesis nula en la prueba de raíz unitaria para determinar qué tan fuertemente los datos de series temporales se ven afectados por una tendencia. Al aceptar la hipótesis nula, aceptamos la evidencia de que los datos de la serie temporal no son estacionarios. Al rechazar la hipótesis nula o aceptar la hipótesis alternativa, aceptamos la evidencia de que los datos de la serie de tiempo son generados por un proceso estacionario. Este proceso también se conoce como tendencia estacionaria. Los valores de la estadística de prueba ADF son negativos. Los valores más bajos de ADF indican un rechazo más fuerte de la hipótesis nula.
"""

def Prueba_Dickey_Fuller(series , column_name):
    print (f'Resultados de la prueba de Dickey-Fuller para columna: {column_name}')
    dftest = adfuller(series, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','No Lags Used','Número de observaciones utilizadas'])
    for key,value in dftest[4].items():
       dfoutput['Critical Value (%s)'%key] = value
    print (dfoutput)
    if dftest[1] <= 0.05:
        print("Conclusion:====>")
        print("Rechazar la hipótesis nula")
        print("Los datos son estacionarios")
    else:
        print("Conclusion:====>")
        print("No se puede rechazar la hipótesis nula")
        print("Los datos no son estacionarios")

Prueba_Dickey_Fuller(df["furniture_price_adjusted"],"furniture_price_adjusted")

"""Podemos observar en el resultado que obtuvismo que la serie no estacionaria, debido a que el p-value es mayor que 5%.

Uno de los objetivo de aplicar la prueba ADF es conocer si nuestra series es estacionaria, al conocer el resultado de la prueba ADF, entonces podemos determinar el siguiente paso. Para nuestro caso, se puede ver en el resultado anterior que la serie no es estacionar, por lo que procederemos al siguiente paso, que es diferenciar nuestra serie.
"""

df1=df.copy()
# Take first difference
df1['furniture_diff'] = df['furniture_price_adjusted'].diff()

# Remove the first data point
df1.dropna(inplace=True)

# Take a look at the head of the dataset
df1.head()

Prueba_Dickey_Fuller(df1["furniture_diff"],"furniture_diff")

"""
> Seasonal ARIMA, es una extensión de ARIMA que admite explícitamente datos de series temporales univariadas con un componente estacional.
> Agrega tres nuevos hiperparámetros para especificar la autorregresión (AR), diferenciación (I) y media móvil (MA) para el componente estacional de la serie, así como un parámetro adicional para el período de la estacionalidad.

> Hay cuatro elementos estacionales que no forman parte de ARIMA que deben configurarse; ellos son:<br>
**P:** orden autorregresivo estacional.<br>
**D:** Orden de diferencia estacional.<br>
**P:** Orden promedio móvil estacional.<br>
**m:** El número de pasos de tiempo para un solo período estacional.<br>"""

plt.rcParams["figure.figsize"] = (12, 8)
a = seasonal_decompose(df1["furniture_diff"], model = "add")
a.plot();

"""## 3.5 División de para entrenamiento y prueba"""

train_data = df[:len(df)-12]
test_data = df[len(df)-12:]
test=test_data.copy()

train_data.shape, test_data.shape

test_data

"""## 3.6 Modelo Auto-Arima

Ejecutemos la función `auto_arima()` para obtener los mejores parametros de p,d,q,P,D,Q

**Nota:** El parámetro $m$ se refiere al número de periodo para cada estacionalidad
* 7 para los días
* 12 para los meses
* 52 para las semanas
* 4 Trimestral
* 1 Anual
"""

# Modelo Auto-Arima
from pmdarima import auto_arima

modelo_auto=auto_arima(train_data,start_p=0,d=1,start_q=0,
          max_p=4,max_d=2,max_q=4, start_P=0,
          D=1, start_Q=0, max_P=2,max_D=1,
          max_Q=2, m=12, seasonal=True,
          error_action='warn',trace=True,
          supress_warnings=True,stepwise=True,
          random_state=20,n_fits=50)
print(modelo_auto)

"""Como podemos ver, el mejor modelo de arima elegido por auto_arima() es SARIMAX(0, 1, 1)x(2, 1, [1, 2], 12)"""

print(modelo_auto.summary())

"""## **3.7 Implementación del Modelo**"""

arima_model = SARIMAX(train_data["furniture_price_adjusted"], order = (0,1,1), seasonal_order = (2,1,2,12))
arima_result = arima_model.fit()
arima_result.summary()

# Gráfico de línea de errores residuales
residuals = pd.DataFrame(arima_result.resid)
residuals.plot(figsize = (16,5));
plt.show();

"""### **Cómo interpretar las gráficas de residuos en el modelo ARIMA**
Repasemos las gráficas de residuos usando stepwise_fit.
"""

plt.style.use('seaborn')
modelo_auto.plot_diagnostics(figsize=(20,8))
plt.show()

"""Entonces, ¿cómo interpretar los diagnósticos de la trama?

Arriba a la izquierda: los errores residuales parecen fluctuar alrededor de una media de cero y tienen una varianza uniforme.

Arriba a la derecha: la gráfica de densidad sugiere una distribución normal con media cero.

Abajo a la izquierda: todos los puntos deben estar perfectamente alineados con la línea roja. Cualquier desviación significativa implicaría que la distribución está sesgada.

Abajo a la derecha: el correlograma, también conocido como gráfico ACF, muestra que los errores residuales no están autocorrelacionados. Cualquier autocorrelación implicaría que existe algún patrón en los errores residuales que no se explican en el modelo. Por lo tanto, deberá buscar más X (predictores) en el modelo.

En general, parece encajar bien. Pronostiquemos.

**Forma 1**
"""

arima_pred = arima_result.predict(start = len(train_data), end = len(df)-1, typ="levels").rename("ARIMA Predictions")
arima_pred

"""Si queremos estimar o predecir otros periodos diferente de los datos de entrenamiento lo podemos hacer de la siguiente manera, escribiendo las fechas de inicio y la fecha final donde queremos hacer la predicción.

**Forma 2**
"""

arima_pred2 = arima_result.predict(start='2015-01-01',end='2025-01-01', typ="levels").rename("ARIMA Predictions")
arima_pred2

"""Grafiquemos ambos casos:"""

plt.style.use('dark_background')
plt.rcParams["figure.figsize"] = (20, 8)

plt.plot(test_data["furniture_price_adjusted"], label="Precio actual")
plt.plot(arima_pred, color="lime", label="Predicciones")
plt.title("Predicción con Modelo Arima", fontsize=30);
plt.xlabel('Meses')
plt.ylabel('')
plt.legend( fontsize=16);
plt.show();

plt.style.use('seaborn')
plt.rcParams["figure.figsize"] = (20, 8)

plt.plot(test_data["furniture_price_adjusted"],color="blue" ,label="Precio actual")
plt.plot(arima_pred2, color="lime", label="Predicciones")
plt.title("Predicción con Modelo Arima", fontsize=30);
plt.xlabel('Meses')
plt.ylabel('')
plt.legend( fontsize=16);
plt.show();

"""Podemos ver que la predicciones tiene un comportamiento parecido a los datos originales."""

evaluacion_metrica(test_data,arima_pred)

"""Guardemos los datos, para luego hacer unas comparaciones con los demás modelos."""

test_data['ARIMA_Predictions'] = arima_pred

test_data

"""# 4. LSTM Forecast

> LSTM significa memoria a corto plazo. Es un modelo o arquitectura que amplía la memoria de las redes neuronales recurrentes. Por lo general, las redes neuronales recurrentes tienen "memoria a corto plazo" en el sentido de que utilizan información anterior persistente para ser utilizada en la red neuronal actual. Esencialmente, la información anterior se utiliza en la presente tarea. Eso significa que no tenemos una lista de toda la información anterior disponible para el nodo neuronal.
> LSTM introduce la memoria a largo plazo en las redes neuronales recurrentes. Mitiga el problema del gradiente de fuga, que es donde la red neuronal deja de aprender porque las actualizaciones de los diversos pesos dentro de una red neuronal dada se vuelven cada vez más pequeñas. Lo hace mediante el uso de una serie de "puertas". Estos están contenidos en bloques de memoria que están conectados a través de capas, así:

![](https://hub.packtpub.com/wp-content/uploads/2018/04/LSTM-696x494.png)

> trabajo LSTM
Hay tres tipos de puertas dentro de una unidad:
Puerta de entrada: escala la entrada a la celda (escritura)
Puerta de salida: escala la salida a la celda (lectura)
Forget Gate: escala el valor de la celda anterior (restablecer)
Cada puerta es como un interruptor que controla la lectura/escritura, incorporando así la función de memoria a largo plazo en el modelo.

## 4.1 Estandarización
Primero escalaremos nuestro datos de entrenamiento y probaremos los datos con MinMaxScaler
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

scaler.fit(train_data)
scaled_train_data = scaler.transform(train_data)

scaled_test_data = scaler.transform(test)

"""Antes de crear el modelo LSTM, debemos crear un objeto Generador de series temporales."""

from keras.preprocessing.sequence import TimeseriesGenerator

n_input = 12
n_features= 1
generator = TimeseriesGenerator(scaled_train_data, scaled_train_data, length=n_input, batch_size=1)

"""## 4.2 Modelación

## 4.2.1 Parámetros clave para LSTM con Keras
Veamos los parámetros clave para ajustar el modelo LSTM.

* **hidden_layer_sizes:** Debe proporcionar una cantidad de capas ocultas y neuronas para cada capa oculta. Por ejemplo, hidden_layer_sizes – (5,3,3) significa que hay tres capas ocultas y el número de neuronas para la capa uno es 5, para la capa dos es 3 y para la capa tres es 3, respectivamente. El valor predeterminado es (100), es decir, una capa oculta con 100 neuronas.
* **activation:** Esta es la función de activación de una capa oculta; hay cuatro funciones de activación disponibles para su uso; el valor predeterminado es "relu".
     * relu: La función de unidad lineal rectificada, devuelve $f(x) = max(0, x)$
     * logística: La función sigmoidea logística, devuelve f(x) = 1 / (1 + exp(-x)).
     * identidad: activación sin operación, útil para implementar un cuello de botella lineal, devuelve f(x) = x
     * tanh: La función tan hiperbólica, devuelve $f(x) = tanh(x)$.
* **solver:** Esto es para optimizar el peso. Hay tres opciones disponibles, la predeterminada es "adam".
     * adam: Optimizador basado en gradiente estocástico propuesto por Diederik Kingma y Jimmy Ba, que funciona bien para un gran conjunto de datos
     * lbfgs: Pertenece a la familia de métodos cuasi-Newton, funciona bien para conjuntos de datos pequeños
     * sgd: Descenso de gradiente estocástico
* **max_iter:** Este es el número máximo de iteraciones para que el solucionador converja, el valor predeterminado es 200.
* **learning_rate_init:** Esta es la tasa de aprendizaje inicial para controlar el tamaño de paso para actualizar los pesos (solo se aplica a los solucionadores sgd/ adam), el valor predeterminado es 0,001.
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

lstm_model = Sequential()
lstm_model.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))
lstm_model.add(Dense(1))
lstm_model.compile(optimizer='adam', loss='mse')

lstm_model.summary()

lstm_model.fit_generator(generator,epochs=100)

losses_lstm = lstm_model.history.history['loss']
#plt.figure(figsize=(12,4))
plt.xticks(np.arange(0,21,1))
plt.plot(range(len(losses_lstm)),losses_lstm);

lstm_predictions_scaled = list()

batch = scaled_train_data[-n_input:]
current_batch = batch.reshape((1, n_input, n_features))

for i in range(len(test_data)):
    lstm_pred = lstm_model.predict(current_batch)[0]
    lstm_predictions_scaled.append(lstm_pred)
    current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)

"""Como saben, escalamos nuestros datos, por eso tenemos que invertirlos para ver predicciones verdaderas."""

lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)

lstm_predictions

"""Guardamos las predicciones para compararla con las verdaderas y el modelo Arima"""

test_data['LSTM_Predictions'] = lstm_predictions

test_data

"""Graficamos las predicciones con los datos originales"""

ai=test_data[["furniture_price_adjusted","LSTM_Predictions"]]
fig = px.line(ai, x=test_data.index, y=ai.columns,title="Predicción con Modelo LSTM", template = "plotly_dark")
fig.show()

"""## 4.3 Evaluación"""

evaluacion_metrica(test_data["furniture_price_adjusted"],test_data["LSTM_Predictions"])

"""# 5. Random Forest
Vamos a descubrirá el modelo Random Forest. Es un modelo fácil de usar y se sabe que tiene un gran rendimiento. Random Forest y el modelo XGBoost, son dos de los algoritmos de aprendizaje automático más utilizados en las aplicaciones modernas.

Existe una gran cantidad de variantes en Random Forests y XGBoost en el mercado, pero si comprende los dos conceptos básicos, será relativamente fácil adaptarse a cualquier variante.

## 5.1 Idea intuitiva detrás de los bosques aleatorios
Random Forest se basa fuertemente en el modelo Decision Tree pero le agrega más complejidad. Como sugiere el nombre, un bosque aleatorio consta de una gran cantidad de árboles de decisión, cada uno de ellos con una ligera variación.

Un bosque aleatorio tiene mucho más rendimiento que un árbol de decisión. Generalmente, un bosque aleatorio puede combinar cientos o incluso miles de modelos de árboles de decisión. Se ajustarán a datos ligeramente diferentes, para que no sean totalmente iguales. Entonces, en resumen, es una gran cantidad de árboles de decisión que hacen predicciones que deberían ser cercanas entre sí, pero no exactamente iguales.

Donde un modelo de aprendizaje automático a veces puede ser incorrecto, es menos probable que la predicción promedio de una gran cantidad de modelos de aprendizaje automático sea incorrecta. Esta idea es la base del aprendizaje conjunto.

En Random Forest, el aprendizaje conjunto se aplica a una repetición de muchos árboles de decisión. El aprendizaje de conjunto se puede aplicar a cualquier combinación de una gran cantidad de modelos de aprendizaje automático. La razón para usar Decision Trees es que se ha demostrado que es un modelo eficaz y fácil de configurar.
"""

df5 = pd.read_csv("/content/drive/MyDrive/_Curso Serie de tiempo/data/df_furniture.csv")
df5.head()

"""### Percentage Change
Recordemos que la unidad original estaba en millones de dólares, que son las ventas de muebles y artículos para el hogar.
Un tipo de diferenciación es el cambio porcentual, que normalmente se aplica a unidades monetarias en montos de precios o ventas en dólares.

Entre cada punto de datos, podemos calcular el cambio porcentual. Tendremos que eliminar el primer punto de datos, ya que se convertirá en un Nan.
"""

df5['furniture_pct_change']= df5['furniture_price_adjusted'].pct_change()
df5.dropna(inplace=True)
df5.head()

df5['furniture_pct_change'].describe()

fig = px.line(df5, x="Month", y="furniture_pct_change",template = "plotly_dark",
              title="Porcentaje de Cambio")
fig.show()

df5['furniture_pct_change'].plot(kind='kde',figsize = (16,5));

"""Ahora es necesario agregar algo de ingeniería de funciones. Agreguemos las variables Año y Mes para tener en cuenta la estacionalidad y las versiones rezagadas de la variable objetivo durante los últimos 12 meses. Se puede esperar que esta tarea sea un poco más difícil, ya que hay una variación más detallada que el modelo necesita aprender.

## 5.2 Feature engineering
"""

# Variables de estacionalidad
df5['Month'] = pd.to_datetime(df5['Month'])
df5['Year'] = df5['Month'].apply(lambda x: x.year)
df5['Mes'] = df5['Month'].apply(lambda x: x.month)
df5.head()

# Agregar un año de datos retrasados
df5['L1'] = df5["furniture_pct_change"].shift(1)
df5['L2'] = df5["furniture_pct_change"].shift(2)
df5['L3'] = df5["furniture_pct_change"].shift(3)
df5['L4'] = df5["furniture_pct_change"].shift(4)
df5['L5'] = df5["furniture_pct_change"].shift(5)
df5['L6'] = df5["furniture_pct_change"].shift(6)
df5['L7'] = df5["furniture_pct_change"].shift(7)
df5['L8'] = df5["furniture_pct_change"].shift(8)
df5['L9'] = df5["furniture_pct_change"].shift(9)
df5['L10'] = df5["furniture_pct_change"].shift(10)
df5['L11'] = df5["furniture_pct_change"].shift(11)
df5['L12'] = df5["furniture_pct_change"].shift(12)

df5.head(13)

"""Ahora que tiene un conjunto de datos, hagamos una división de prueba de entrenamiento y ajustemos el Random Forest con los hiperparámetros predeterminados."""

df5 = df5.dropna()
df5.head()

df5= df5.set_index("Month")
df5.head()

"""## 5.3 Modelación"""

# dividir en variables rezagadas (características) y datos de series de tiempo originales (objetivo)
X2= df5.iloc[:,2:] # dividir todas las filas y comenzar con la columna 0 y subir hasta la última columna, pero sin incluirla
y2 = df5.iloc[:,1] # dividir todas las filas y la última columna, esencialmente separando la columna 't'

X2.head(3)

y2.head()

# Target Train-Test split
from pandas import read_csv

Y2 = y2
traintarget_size = int(len(Y2) * 0.80)   # Set split
train_target, test_target = Y2[0:traintarget_size], Y2[traintarget_size:len(Y2)]

print('Observaciones para el objetivo: %d' % (len(Y2)))
print('Observaciones de entrenamiento para el objetivo: %d' % (len(train_target)))
print('Observaciones de prueba para el objetivo: %d' % (len(test_target)))

# Features Train-Test split

trainfeature_size = int(len(X2) * 0.80)
train_feature, test_feature = X2[0:trainfeature_size], X2[trainfeature_size:len(X2)]
print('Observaciones para la característica: %d' % (len(X2)))
print('Observaciones de entrenamiento para la característica: %d' % (len(train_feature)))
print('Observaciones de prueba para la característica: %d' % (len(test_feature)))

# Creando Train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.2,  shuffle=False)

# Random Forest
from sklearn.ensemble import RandomForestRegressor

# Creamos el modelo con 500 árboles
rfr = RandomForestRegressor(n_estimators=500)

# Entrenamos el modelo
rfr.fit(train_feature, train_target)

# Hacemos las predicciones
fcst = rfr.predict(test_feature)

b=pd.DataFrame({"Actual":test_target, "Random Forest":fcst})
b

#b.set_index("2018-08-01")
#b=b.loc['2018-08-01':"2019-07-01"]

fig = px.line(b, x=b.index, y=b.columns,template = "plotly_dark",
              title="Predicción con Modelo Random Forest")
fig.show()

"""Parece que Random Forest se ajusto bien, los valores pronosticados siguen los valores reales.

## 5.4 Evaluamos el modelo
"""

evaluacion_metrica(test_target,fcst)

"""# 6. Prophet Forecast

> Prophet es un procedimiento para pronosticar datos de series temporales basado en un modelo aditivo en el que las tendencias no lineales se ajustan a la estacionalidad anual, semanal y diaria, además de los efectos de las vacaciones. Funciona mejor con series temporales que tienen fuertes efectos estacionales y varias temporadas de datos históricos. Prophet es resistente a los datos faltantes y los cambios en la tendencia, y por lo general maneja bien los valores atípicos.
"""

import pandas as pd
from prophet import Prophet

df6 = df.copy()
df6.head()

df6=df6.reset_index()

df_fb=df6.rename(columns={"Month":"ds", "furniture_price_adjusted":"y"} )
df_fb.head()

train_data_pr = df_fb.iloc[:len(df6)-12]
test_data_pr = df_fb.iloc[len(df6)-12:]

"""## 6.1 Modelación"""

from prophet import Prophet

m = Prophet()

# Hacemos el entrenamiento
m.fit(train_data_pr)

future = m.make_future_dataframe(periods=12,freq='MS')
prophet_pred = m.predict(future)

prophet_pred

prophet_pred = pd.DataFrame({"Date" : prophet_pred[-12:]['ds'], "Pred" : prophet_pred[-12:]["yhat"]})

prophet_pred = prophet_pred.set_index("Date")

prophet_pred.index.freq = "MS"

prophet_pred

test_data["Prophet_Predictions"] = prophet_pred['Pred'].values

test_data.head()

a=test_data[["furniture_price_adjusted","Prophet_Predictions"]]
fig = px.line(a, x=test_data.index, y=a.columns,template = "plotly_dark",
              title="Predicción con Modelo Prophet")
fig.show()

"""## 6.2 Evaluación"""

evaluacion_metrica(test_data["furniture_price_adjusted"],test_data["Prophet_Predictions"])

"""## **Resumen**"""

fig = px.line(test_data, x=test_data.index, y=test_data.columns,template = "plotly_dark",
              title="Predicciones")
fig.show()

plt.figure(figsize=(16,9))
plt.plot_date(test_data.index, test_data["furniture_price_adjusted"],label="Original", linestyle="-")
plt.plot_date(test_data.index, test_data["ARIMA_Predictions"], label="Arima",linestyle="-.")
plt.plot_date(test_data.index, test_data["LSTM_Predictions"],label="LSTM", linestyle="--")
plt.plot_date(test_data.index, test_data["Prophet_Predictions"], label="Prophet",linestyle=":")
plt.legend(fontsize=12)
plt.title("Prediciones de los Diferentes Modelos", fontsize=22)
plt.show();

test_data

"""##Conclusión:
Analizar y comparar los resultados obtenidos con los modelos ARIMA, AutoARIMA y la red neuronal permitirá determinar qué enfoque proporciona las mejores predicciones de precios de muebles. Estas predicciones pueden ser valiosas para la mueblería en términos de optimización de precios, gestión de inventario y toma de decisiones estratégicas relacionadas con las estrategias de precios. Es importante tener en cuenta que el desempeño y la adecuación de los modelos dependerán de la disponibilidad y calidad de los datos de precios históricos de la mueblería.
"""
